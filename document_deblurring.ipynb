{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-rrQU_2P1ha"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orig_imgs = '/content/drive/MyDrive/small_orig_imgs'\n",
        "blur_imgs = '/content/drive/MyDrive/small_blur_imgs'"
      ],
      "metadata": {
        "id": "FxEQW4rwQRy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the image and convert into\n",
        "# numpy array\n",
        "def create_array(folder_dir):\n",
        "  arr = []\n",
        "  oslist = sorted(os.listdir(orig_imgs))\n",
        "  for image in oslist:\n",
        "    img = Image.open(folder_dir + \"/\" + image)\n",
        "    img = img.resize((64,128))\n",
        "    data = np.asarray(img)\n",
        "    arr.append(data)\n",
        "  return np.array(arr)"
      ],
      "metadata": {
        "id": "_uATW6f0QTJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = create_array(orig_imgs)\n",
        "y = create_array(blur_imgs)\n",
        "\n",
        "tensor_x = torch.Tensor(x).permute(0, 3, 1, 2)\n",
        "tensor_y = torch.Tensor(y).permute(0, 3, 1, 2)\n",
        "\n",
        "my_dataset = TensorDataset(tensor_x,tensor_y)\n",
        "my_dataloader = DataLoader(my_dataset)"
      ],
      "metadata": {
        "id": "B7BJCvrYQTLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "                  #input_shape=(3, 512,1024)\n",
        "                  nn.Conv2d(3, 10, kernel_size=3, stride=2, padding=1),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Conv2d(10, 25, kernel_size=3, stride=2, padding=1),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Conv2d(25, 50, kernel_size=3, stride=2, padding=1)\n",
        "                )\n",
        "        self.linear = nn.Linear(8*16*50, ro)\n",
        "        # 64 -> 32 -> 16 -> 8\n",
        "        # 128 -> 64 -> 32 -> 16\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gqqsdJcOQTQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear = nn.Linear(128, 8*16*3)\n",
        "        self.model = nn.Sequential(\n",
        "                  nn.ConvTranspose2d(3, 50, kernel_size=4, stride=2, padding=1),\n",
        "                  nn.ReLU(),\n",
        "                  nn.ConvTranspose2d(50, 25, kernel_size=4, stride=2, padding=1),\n",
        "                  nn.ReLU(),\n",
        "                  nn.ConvTranspose2d(25, 10, kernel_size=4, stride=2, padding=1),\n",
        "                  nn.ReLU(),\n",
        "                  nn.ConvTranspose2d(10, 3, kernel_size=3, padding=1)\n",
        "                )\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = torch.reshape(x, (-1,3,16,8))\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pVgVghrJQTS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Encoder()\n",
        "summary(model1, input_size = (3, 128, 64), batch_size=-1)"
      ],
      "metadata": {
        "id": "eX8BCislQTVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Decoder()\n",
        "summary(model2, input_size = (1,128), batch_size=-1)"
      ],
      "metadata": {
        "id": "q6H9qdSMQTYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))"
      ],
      "metadata": {
        "id": "58cYOC2GQTaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = AutoEncoder()\n",
        "model = AutoEncoder().to(device)\n",
        "\n",
        "# Validation using MSE Loss function\n",
        "loss_function = torch.nn.MSELoss()\n",
        " \n",
        "# Using an Adam Optimizer with lr = 0.1\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr = 1e-1,\n",
        "                             weight_decay = 1e-8)"
      ],
      "metadata": {
        "id": "zr299rbnQ1se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "    print(\"epoch no:\", epoch)\n",
        "    for (image, _) in my_dataloader:\n",
        "\n",
        "      # Output of Autoencoder\n",
        "      deblurred_image = model(image)\n",
        "       \n",
        "      # Calculating the loss function\n",
        "      loss = loss_function(deblurred_image, image)\n",
        "       \n",
        "      # The gradients are set to zero,\n",
        "      # the gradient is computed and stored.\n",
        "      # .step() performs parameter update\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "       \n",
        "      # Storing the losses in a list for plotting\n",
        "      losses.append(loss)\n",
        " \n",
        "# Defining the Plot Style\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        " \n",
        "for i in range(len(losses)):\n",
        "  losses[i] = losses[i]\n",
        "# Plotting the last 100 values\n",
        "plt.plot(losses[-100:])"
      ],
      "metadata": {
        "id": "tbShdfUdQ1u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(image):\n",
        "  plt.imshow(item[0])\n",
        " \n",
        "for i, item in enumerate(deblurred_image):\n",
        "  plt.imshow(item[0])"
      ],
      "metadata": {
        "id": "gQU6Rze7Q1zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AL7RwYvKRgPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}